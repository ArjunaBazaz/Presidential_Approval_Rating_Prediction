import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import statsmodels.api as sm
import matplotlib.pyplot as plt

# load approval ratings
file_paths = [
    'approval_rating_biden_1_updated.csv',
    'approval_rating_bushjr_1_2_updated.csv',
    'approval_rating_bushsr_1_updated.csv',
    'approval_rating_carter_1_updated.csv',
    'approval_rating_clinton_1_2_updated.csv',
    'approval_rating_eisenhower_1_2_updated.csv',
    'approval_rating_ford_1_updated.csv',
    'approval_rating_johnson_1_2_updated.csv',
    'approval_rating_kennedy_1_updated.csv',
    'approval_rating_nixon_1_2_updated.csv',
    'approval_rating_obama_1_2 (1)_updated.csv',
    'approval_rating_reagan_1_2 (1)_updated.csv',
    'approval_rating_roosevelt_3_4 (1)_updated.csv',
    'approval_rating_truman_1 (1)_updated.csv',
    'approval_rating_trump_1 (1)_updated.csv'
]

# read/concat
approval = pd.concat([pd.read_csv(f, sep='\t') for f in file_paths], ignore_index=True)
approval.columns = ['Start Date', 'End Date', 'Approving', 'Disapproving', 'Unsure/NoData', 'Candidate']
approval['Start Date'] = pd.to_datetime(approval['Start Date'], format='%m/%d/%Y')

# load econ data
gdp = pd.read_csv('./real_GDP_per_capita.csv', index_col=False)
gdp_change = pd.read_csv('./real_GDP_per_capita_daily_change.csv', index_col=False)
income = pd.read_csv('./median_household_income.csv', index_col=False)
income_change = pd.read_csv('./median_household_income_daily_change.csv', index_col=False)
sp500 = pd.read_csv('./sp500_historical_data.csv', index_col=False)
sp500_change = pd.read_csv('./sp500_daily_change.csv', index_col=False)
unemployment = pd.read_csv('./unemployment_rate.csv', index_col=False)

# rename columns for clean names
gdp.rename(columns={'A939RX0Q048SBEA': 'GDP', 'observation_date': 'observation_date'}, inplace=True)
gdp_change.rename(columns={'A939RX0Q048SBEA': 'GDP_Change', 'observation_date': 'observation_date'}, inplace=True)

income.rename(columns={'MEHOINUSA672N': 'Income', 'observation_date': 'observation_date'}, inplace=True)
income_change.rename(columns={'MEHOINUSA672N': 'Income', 'Change': 'Income_Change', 'observation_date': 'observation_date'}, inplace=True)

unemployment.rename(columns={'UNRATE': 'Unemployment', 'observation_date': 'observation_date'}, inplace=True)
sp500.rename(columns={'Close': 'SP500_Close'}, inplace=True)
sp500_change.rename(columns={'Close_Change': 'SP500_Close_Change'}, inplace=True)

# make dates datetime format
gdp['observation_date'] = pd.to_datetime(gdp['observation_date'])
gdp_change['observation_date'] = pd.to_datetime(gdp_change['observation_date'])
income['observation_date'] = pd.to_datetime(income['observation_date'])
income_change['observation_date'] = pd.to_datetime(income_change['observation_date'])
sp500['Date'] = pd.to_datetime(sp500['Date'], utc=True).dt.tz_localize(None)
sp500_change['Date'] = pd.to_datetime(sp500_change['Date'], utc=True).dt.tz_localize(None)
unemployment['observation_date'] = pd.to_datetime(unemployment['observation_date'])

# sort values for merge_asof()
approval = approval.sort_values(by='Start Date')
gdp = gdp.sort_values(by='observation_date')
gdp_change = gdp_change.sort_values(by='observation_date')
income = income.sort_values(by='observation_date')
income_change = income_change.sort_values(by='observation_date')
unemployment = unemployment.sort_values(by='observation_date')
sp500 = sp500.sort_values(by='Date')
sp500_change = sp500_change.sort_values(by='Date')

# merge 
df = pd.merge_asof(approval, gdp, left_on='Start Date', right_on='observation_date', direction='backward')
df.drop(columns=['observation_date'], inplace=True)

df = pd.merge_asof(df, gdp_change, left_on='Start Date', right_on='observation_date', direction='backward')
df.drop(columns=['observation_date'], inplace=True)

df = pd.merge_asof(df, income, left_on='Start Date', right_on='observation_date', direction='backward')
df.drop(columns=['observation_date'], inplace=True)

# Drop  Date After Merge 
df = pd.merge_asof(df, income_change, left_on='Start Date', right_on='observation_date', direction='backward')
df.drop(columns=['observation_date'], inplace=True) 

df = pd.merge_asof(df, unemployment, left_on='Start Date', right_on='observation_date', direction='backward')
df.drop(columns=['observation_date'], inplace=True)

df = pd.merge_asof(df, sp500, left_on='Start Date', right_on='Date', direction='backward')
df.drop(columns=['Date'], inplace=True)

df = pd.merge_asof(df, sp500_change, left_on='Start Date', right_on='Date', direction='backward')
df.drop(columns=['Date'], inplace=True)

# take care of NaNs
df.ffill(inplace=True)
df.fillna(method='bfill', inplace=True)  # Backup fill
df.replace([np.inf, -np.inf], np.nan, inplace=True)


# drop rows with missing target values
df.dropna(subset=['Approving'], inplace=True)

# make interaction terms
df['Unemployment_Stock'] = df['Unemployment'] * df['SP500_Close']
df['GDP_Unemployment'] = df['GDP'] * df['Unemployment']

print(df.columns)

# use GDP, Unemployment, and SP500_Close as predictors (remove Income and GDP_Unemployment)
X = df[['GDP', 'Unemployment', 'SP500_Close']]  # focus on the most relevant variables-- but maybe  change??

# normalize  predictors 
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# constant term for intercept
X_scaled = sm.add_constant(X_scaled)

# fit model
model = sm.OLS(y, X_scaled).fit()
print(model.summary())

# eval model performance
y_pred = model.predict(X_scaled)
mae = mean_absolute_error(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = r2_score(y, y_pred)

print("\nModel Performance:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R-squared: {r2:.2f}")
